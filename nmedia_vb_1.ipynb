{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fead3ed8",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "This tutorial accompanies the \"Introduction to Bayesian Inference\" video in the NMedIA \"Essentials of Medical Imaging Analysis\" course. Bayesian inference is commonly used in modelling scenarios. The idea of this tutorial is to allow you to adjust different parameters in a problem where Bayesian inference is used for model fitting. We hope this will help you build intuition and assist you if you come across algorithms using Bayesian inference in practice.\n",
    "\n",
    "To use this notebook, hit \"Cell\" and then \"Run All\" to run all of the cells. This will run the cell below which will hide the code that produces the outputs. (If you do want to see the code, you can click the toggle which will appear below which says \"Click here to show/hide code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae75c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to show/hide code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, GridspecLayout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some plotting functions\n",
    "colours = {\"ground_truth\": \"#4daf4a\",\n",
    "           \"empirical\": \"#f781bf\",\n",
    "           \"exact\": \"#377eb8\",\n",
    "           \"approx\": \"#ff7f00\",\n",
    "           \"prior\": \"#a65628\"}\n",
    "linestyles = {\"ground_truth\": \"solid\",\n",
    "              \"empirical\": \"dashdot\",\n",
    "              \"exact\": \"dashed\",\n",
    "              \"approx\": \"dotted\",\n",
    "              \"prior\": (0, (3, 5, 1, 5, 1, 5))}\n",
    "\n",
    "def get_contours(x, mu, std):\n",
    "    return (1 / np.sqrt(np.pi * std)) * np.exp(-0.5 * np.square((x - mu) / (std**2)))\n",
    "\n",
    "def plot_contours(ax, *args):\n",
    "    for arg in args:\n",
    "        x, y, c, l, ls = arg\n",
    "        ax.plot(x, y, color=c, linestyle=ls, label=l)\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "def plot_tramlines(ax, *args):\n",
    "    for arg in args:\n",
    "        mu, std, label = arg\n",
    "        ax.axvline(mu - 2*std, color=colours[label], linestyle=\"--\")\n",
    "        ax.axvline(mu, color=colours[label])\n",
    "        ax.axvline(mu + 2*std, color=colours[label], linestyle=\"--\")\n",
    "\n",
    "def get_x_range(*args):\n",
    "    xmin = None\n",
    "    xmax = None\n",
    "    for arg in args:\n",
    "        mu, std = arg\n",
    "        l = mu - 3 * std\n",
    "        if xmin:\n",
    "            if l < xmin:\n",
    "                xmin = l\n",
    "        else:\n",
    "            xmin = l\n",
    "        u = mu + 3 * std\n",
    "        if xmax:\n",
    "            if u > xmax:\n",
    "                xmax = u\n",
    "        else:\n",
    "            xmax = u\n",
    "    return np.linspace(xmin, xmax, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73465b9",
   "metadata": {},
   "source": [
    "# Example Scenario\n",
    "\n",
    "Like all good tutorials, we'll assume our data was sampled from a Normal distribution. Our goal is to estimate the parameters of the Normal distribution from which our data was sampled, i.e. the mean and standard deviation.\n",
    "\n",
    "## Sample some data\n",
    "\n",
    "Below, we sample some data from a 1-dimensional Normal distribution and plot their histogram. Feel free to play with the value of the mean, $\\mu$, and the standard deviation, $\\sigma$, and see how this changes the data.\n",
    "\n",
    "Things to note/try:\n",
    "* For large values of $N$, you should see the typical bell-curve shape of the Normal distribution.\n",
    "* If you change $\\mu$, the centre of the data will move (i.e. the centre value on the $x$ axis will move).\n",
    "* If you increase $\\sigma$, the spread of the data will increase (i.e. the range on the $x$ axis will increase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(N, mu, std):\n",
    "    w.value = np.random.normal(mu, std, [N])\n",
    "\n",
    "def display_data(ax, data):\n",
    "    ax.hist(w.value, bins=10, color='g', align='left')\n",
    "    plt.show()\n",
    "    \n",
    "def update_plot(change):\n",
    "    ax0.clear()\n",
    "    display_data(ax0, data=w.value)\n",
    "\n",
    "# set up figure\n",
    "fig0, ax0 = plt.subplots(num=0)\n",
    "\n",
    "# get our widgets\n",
    "N = widgets.IntSlider(value=50, min=5, max=100, step=5, description=\"N\", continuous_update=False)\n",
    "mu = widgets.IntSlider(value=0, min=-10, max=10, step=1, description=\"$\\mu$\", continuous_update=False)\n",
    "std = widgets.FloatSlider(value=1., min=0.1, max=10, step=0.1, description=\"$\\sigma$\", continuous_update=False)\n",
    "\n",
    "w = interactive(get_data, N=N, mu=mu, std=std);\n",
    "display(w);\n",
    "\n",
    "# display our data\n",
    "display_data(ax0, w.value);\n",
    "\n",
    "# watch for a change in our widgets. This is messy, is there a nicer way to do it?\n",
    "[w.children[n].observe(update_plot, 'value') for n in range(len(w.children))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f79a6",
   "metadata": {},
   "source": [
    "# Empirical Mean and Standard Deviation\n",
    "\n",
    "The first thing we might think to do is simply get the mean and the standard deviation of our observations.\n",
    "\n",
    "If you play around with the values of the parameters $N$, $\\mu$ and $\\sigma$ you should notice:\n",
    "* Increasing the number of observations, $N$, drives our empirical approximation closer to the ground truth;\n",
    "* Similarly, deacreasing the standard deviation, $\\sigma$, of our distribution generally results in a better approximation;\n",
    "* If we have a small number of observations, $N$, and a large standard deviation, $\\sigma$, our empirical approximation can be quite poor. We don't have any measure of confidence in our estimate however so this can be difficult to diagnose in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_empirical_contour(change):\n",
    "    emp_mean, emp_std = [np.mean(w.value), np.std(w.value)]\n",
    "    x = get_x_range((mu.value, std.value), (emp_mean, emp_std))\n",
    "    data_contour = get_contours(x, mu.value, std.value)\n",
    "    emp_contour = get_contours(x, emp_mean, emp_std)\n",
    "    ax1.clear()\n",
    "    plot_contours(ax1,\n",
    "                  (x, data_contour, colours[\"ground_truth\"], \"Ground Truth\", linestyles[\"ground_truth\"]),\n",
    "                  (x, emp_contour, colours[\"approx\"], \"Empirical\", linestyles[\"approx\"]))\n",
    "\n",
    "emp_mean, emp_std = [np.mean(w.value), np.std(w.value)]\n",
    "display(w);\n",
    "\n",
    "fig1, ax1 = plt.subplots(num=1)\n",
    "x = get_x_range((mu.value, std.value), (emp_mean, emp_std))\n",
    "data_contour = get_contours(x, mu.value, std.value)\n",
    "emp_contour = get_contours(x, emp_mean, emp_std)\n",
    "plot_contours(ax1,\n",
    "              (x, data_contour, colours[\"ground_truth\"], \"Ground Truth\", linestyles[\"ground_truth\"]),\n",
    "              (x, emp_contour, colours[\"approx\"], \"Empirical\", linestyles[\"approx\"]))\n",
    "[w.children[n].observe(update_empirical_contour, 'value') for n in range(len(w.children))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7acd8b",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "Suppose we also want to express our confidence in our parameter estimates, i.e. how confident are we in our estimate of the mean, $\\mu$? The simple method above can't do this. It only gives us estimates of $\\mu$ and $\\sigma$.\n",
    "\n",
    "It also doesn't let us include prior knowledge about the ground truth distribution. This prior knowledge can include biophysical \"soft\" constraints where we have a reasonable suspicion that the value of a parameter lies within certain bounds or spatial regularisation where we believe our signal follows a certain pattern in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef13ec",
   "metadata": {},
   "source": [
    "## Set up our prior\n",
    "\n",
    "In the following model, we have 4 parameters. These are:\n",
    "* $m_0$ = Our prior knowledge of the mean;\n",
    "* $v_0$ = Our prior confidence in our prior mean;\n",
    "* $\\beta_0$ = Our prior on the mean of the precision;\n",
    "* $\\sigma_0$ = How confident we are in our prior on the precision.\n",
    "\n",
    "If you vary the values of the prior mean, $m_0$, and prior precision, $\\beta_0$, away from the values of the parameters of the data distribution, $\\mu$ and $\\frac{1}{\\sigma^2}$ respectively, you will notice that the shape of the prior no longer matches the shape of the data distribution. This isn't necessarily a bad thing. We can use large variances on those prior values to indicate that we aren't particularly confident in their values, which will allow them to vary more freely when fitting the model. This is known as an uninformative prior. With sufficient data, the posterior distribution should still converge to the correct answer. If our prior is wrong and we are over-confident in it, we will need more data to converge to the correct answer. We'll see examples of these scenarios later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00218c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prior(m0, v0, beta_mean0, beta_var0):\n",
    "    print(m0, v0, beta_mean0, beta_var0)\n",
    "\n",
    "def update_prior_contour(change):\n",
    "    x = get_x_range((m0.value, beta_mean0.value), (mu.value, std.value))\n",
    "    data_contour = get_contours(x, mu.value, std.value)\n",
    "    prior_contour = get_contours(x, m0.value, np.sqrt(1/beta_mean0.value))\n",
    "    ax2.clear()\n",
    "    plot_contours(ax2,\n",
    "                  (x, data_contour, colours[\"ground_truth\"], \"Ground Truth\", linestyles[\"ground_truth\"]),\n",
    "                  (x, prior_contour, colours[\"prior\"], \"Prior\", linestyles[\"prior\"]))\n",
    "\n",
    "# create widgets for the prior values of our NormalGamma distribution\n",
    "m0 = widgets.IntSlider(value=0, min=-10, max=10, step=1, description=\"$m_0$\", continuous_update=False)\n",
    "v0 = widgets.IntSlider(value=1000, min=1, max=1000, step=1, description=\"$v_0$\", continuous_update=False)\n",
    "beta_mean0 = widgets.IntSlider(value=1, min=1, max=100, step=1, description=\"Beta mean\", continuous_update=False)\n",
    "beta_var0 = widgets.IntSlider(value=1000, min=1, max=1000, step=1, description=\"Beta var\", continuous_update=False)\n",
    "prior_widget = interactive(print_prior, m0=m0, v0=v0, beta_mean0=beta_mean0, beta_var0=beta_var0)\n",
    "\n",
    "fig2, ax2 = plt.subplots(num=2)\n",
    "x = get_x_range((m0.value, beta_mean0.value), (mu.value, std.value))\n",
    "prior_contour = get_contours(x, m0.value, np.sqrt(1/beta_mean0.value))\n",
    "data_contour = get_contours(x, mu.value, std.value)\n",
    "plot_contours(ax2,\n",
    "              (x, prior_contour, colours[\"prior\"], \"Prior\", linestyles[\"prior\"]),\n",
    "              (x, data_contour, colours[\"ground_truth\"], \"Ground Truth\", linestyles[\"ground_truth\"]))\n",
    "\n",
    "grid = GridspecLayout(1, 2)\n",
    "grid[0, 0] = w\n",
    "grid[0, 1] = prior_widget\n",
    "display(grid);\n",
    "\n",
    "[w.children[n].observe(update_prior_contour, 'value') for n in range(len(w.children))];\n",
    "[prior_widget.children[n].observe(update_prior_contour, 'value') for n in range(len(prior_widget.children))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcaf9f",
   "metadata": {},
   "source": [
    "## Exact Solution\n",
    "\n",
    "In a lot of cases, inferring the posterior distribution of our parameters is an intractable problem. In the case we're considering here however, we can find the exact solution. The solid verticle line indicates the mean of our posterior distribution and the dashed verticle lines indicate the 95% confidence region of this mean.\n",
    "\n",
    "We'll see later how we can still get a good approximation of the posterior in cases where our problem is intractable. Also, because we have the exact solution, we can compare the approximated solution with the exact solution to see how good our approximation is!\n",
    "\n",
    "Things to try below:\n",
    "* Set the prior mean, $m_0$, and prior precision, $\\beta_0$ equal to the mean, $\\mu$, and standard deviation, $\\sigma$, of the data distribution. As you increase the number of samples, $N$, you should notice that the model fit given by the dashed blue line generally gets better;\n",
    "* With $m_0$ and $\\beta_0$ at the \"correct\" values, decrease $v_0$ and beta var equal to 1 to indicate we are confident in their values. You might not notice much difference to the model fit when $m_0$ and $\\beta_0$ are correct;\n",
    "* With a small number of data samples, $N=5$, set $v_0$ and $beta var$ to their original values of 1000 to indicate we don't have much confidence in our prior. Try varying the value of $m_0$ away from the true value, $\\mu$. You should find it doesn't change the exact solution too much. This is because the high value of $v_0$ means we don't give much weight to the value of $m_0$;\n",
    "* Now try increasing our confidence in this incorrect value of $m_0$ by setting $v_0=1$. Any changes to $m_0$ should have much more of an impact on the final solution because we have indicated we trust our prior more. You should see that the tramlines indicating the 95% confidence region of our estimated mean have gotten significantly wider, indicating that our model fit isn't particularly confident in our estimated mean. This is an advantage over the empirical method above where we couldn't see how much we trusted our mean estimate;\n",
    "* Now try increasing the number of data samples. You should see that more data will still improve our model fit, even if we have an over-confident and inaccurate prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6391da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(data, m0, v0, beta_mean0, beta_var0, iterations=10):\n",
    "    N = len(data)\n",
    "    s1 = np.sum(data)\n",
    "    s2 = np.sum(np.square(data))\n",
    "    xbar = np.mean(data)\n",
    "    b0 = beta_var0/beta_mean0\n",
    "    c0 = (beta_mean0**2)/beta_var0\n",
    "    \n",
    "    vN = v0 / (1 + N*v0)\n",
    "    mN = ((m0/v0) + s1) * vN\n",
    "    cN = c0 + (N/2)\n",
    "    d = ((m0**2)/v0) - ((mN**2)/vN) + s2\n",
    "    bN = 1 / ((1/b0) + d/2)\n",
    "    beta_mean = bN * cN\n",
    "    beta_var = (bN**2) * cN\n",
    "    return mN, vN, beta_mean, beta_var\n",
    "\n",
    "def update_exact_contour(change):\n",
    "    ax3.clear()\n",
    "    mE, vE, beta_meanE, beta_varE = exact_solution(w.value, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "    stdE = np.sqrt(1 / beta_meanE)\n",
    "    x = get_x_range((mE, stdE), (mu.value, std.value))\n",
    "    exact_contour = get_contours(x, mE, stdE)\n",
    "    data_contour = get_contours(x, mu.value, std.value)\n",
    "    plot_contours(ax3,\n",
    "                  (x, exact_contour, colours[\"exact\"], \"Exact\", linestyles[\"exact\"]),\n",
    "                  (x, data_contour, colours[\"ground_truth\"], \"Ground truth\", linestyles[\"ground_truth\"]))\n",
    "    bE = beta_varE / beta_meanE\n",
    "    cE = (beta_meanE**2) / beta_varE\n",
    "    plot_tramlines(ax3, (mE, np.sqrt(vE / (bE * (cE - 1))), \"exact\"))\n",
    "\n",
    "mE, vE, beta_meanE, beta_varE = exact_solution(w.value, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "stdE = np.sqrt(1 / beta_meanE)\n",
    "x = get_x_range((mE, stdE), (mu.value, std.value))\n",
    "exact_contour = get_contours(x, mE, stdE)\n",
    "data_contour = get_contours(x, mu.value, std.value)\n",
    "\n",
    "fig3, ax3 = plt.subplots(num=3)\n",
    "plot_contours(ax3,\n",
    "              (x, exact_contour, colours[\"exact\"], \"Exact\", linestyles[\"exact\"]),\n",
    "              (x, data_contour, colours[\"ground_truth\"], \"Ground truth\", linestyles[\"ground_truth\"]))\n",
    "bE = beta_varE / beta_meanE\n",
    "cE = (beta_meanE**2) / beta_varE\n",
    "plot_tramlines(ax3, (mE, np.sqrt(vE / (bE * (cE - 1))), \"exact\"))\n",
    "\n",
    "display(grid);\n",
    "\n",
    "[w.children[n].observe(update_exact_contour, 'value') for n in range(len(w.children))];\n",
    "[prior_widget.children[n].observe(update_exact_contour, 'value') for n in range(len(prior_widget.children))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99b870",
   "metadata": {},
   "source": [
    "## Variational Bayes\n",
    "\n",
    "As mentioned above, the exact posterior for a probabilistic model is often intractable. This means that we are often forced to use approximations to enable us to carry out Bayesian inference.\n",
    "\n",
    "Variational Bayes is a popular algorithm for inferring posterior distributions. It uses the mean field assumption. This means it assumes that the parameters to be estimated are independent of eachother. In our example here, it means we assume the mean and standard deviation of our distribution are independent whereas in our exact solution they covary. If you play around with the values in the sliders making some of the same adjustments as above, you should find it generally performs quite similarly to the exact solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a51195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vb_solution(data, m0, v0, beta_mean0, beta_var0, iterations=100):\n",
    "    N = len(data)\n",
    "    s1 = np.sum(data)\n",
    "    s2 = np.sum(np.square(data))\n",
    "    b0 = beta_var0 / beta_mean0\n",
    "    c0 = (beta_mean0**2)/beta_var0\n",
    "    m, v, b, c = m0, v0, b0, c0\n",
    "    for i in range(iterations):\n",
    "        m = (m0 + v0 * b * c * s1) / (1 + N * v0 * b * c)\n",
    "        v = v0 / (1 + N * v0 * b * c)\n",
    "        x = s2 - 2*s1*m + N*(m**2 + v)\n",
    "        b = 1 / (1/b0 + x/2)\n",
    "        c = N/2 + c0\n",
    "    beta_mean = b * c\n",
    "    beta_var = (b**2) * c\n",
    "    return m, v, beta_mean, beta_var\n",
    "    \n",
    "def update_vb_contour(change):\n",
    "    ax4.clear()\n",
    "    mVB, vVB, beta_meanVB, beta_varVB = vb_solution(w.value, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "    stdVB = np.sqrt(1 / beta_meanVB)\n",
    "    x = get_x_range((mVB, stdVB), (mu.value, std.value))\n",
    "    vb_contour = get_contours(x, mVB, stdVB)\n",
    "    data_contour = get_contours(x, mu.value, std.value)\n",
    "    plot_contours(ax4,\n",
    "                  (x, vb_contour, colours[\"approx\"], \"Approximation\", linestyles[\"approx\"]),\n",
    "                  (x, data_contour, colours[\"ground_truth\"], \"Ground truth\", linestyles[\"ground_truth\"]))\n",
    "    plot_tramlines(ax4, (mVB, np.sqrt(vVB), \"approx\"))\n",
    "\n",
    "\n",
    "        \n",
    "mVB, vVB, beta_meanVB, beta_varVB = vb_solution(w.value, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "stdVB = np.sqrt(1 / beta_meanVB)\n",
    "x = get_x_range((mVB, stdVB), (mu.value, std.value))\n",
    "vb_contour = get_contours(x, mVB, stdVB)\n",
    "data_contour = get_contours(x, mu.value, std.value)\n",
    "\n",
    "fig4, ax4 = plt.subplots(num=4)\n",
    "plot_contours(ax4,\n",
    "              (x, vb_contour, colours[\"approx\"], \"Approximation\", linestyles[\"approx\"]),\n",
    "              (x, data_contour, colours[\"ground_truth\"], \"Ground truth\", linestyles[\"ground_truth\"]))\n",
    "plot_tramlines(ax4, (mVB, np.sqrt(vVB), \"approx\"))\n",
    "\n",
    "display(grid);\n",
    "\n",
    "[w.children[n].observe(update_vb_contour, 'value') for n in range(len(w.children))];\n",
    "[prior_widget.children[n].observe(update_vb_contour, 'value') for n in range(len(prior_widget.children))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9118a5",
   "metadata": {},
   "source": [
    "# How good is our approximation?\n",
    "\n",
    "Above, we mentioned that an advantage of this example is that we can compare the quality of fit of the approximate solution with that of the exact solution. In the graph below, we compare the estimates of the mean from both methods as well as our confidence in that mean.\n",
    "\n",
    "With $N=5$, you should notice that the approximation is generally a little overconfident compared to the exact solution. This can be seen by the approximation's mean distribution having a slightly higher peak and more narrow distribution.\n",
    "\n",
    "Try slowly increasing the number of samples $N$ from $5$ to $10$, $15$, $20$, $25$ and $50$. You should see that with more data samples, the approximate solution's mean and confidence in that mean gradually approaches that of the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158baa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(x, mean, std):\n",
    "    return (1 / (std * np.sqrt(np.pi))) * np.exp(-0.5 * np.square((x - mean) / std))\n",
    "\n",
    "def plot_vb_and_exact_contours(ax):\n",
    "    meanE, standard_devE = [], []\n",
    "    meanVB, standard_devVB = [], []\n",
    "    # average over 100 runs\n",
    "    for i in range(100):\n",
    "        data = np.random.normal(mu.value, std.value, [N.value])\n",
    "\n",
    "        # get exact and VB solutions\n",
    "        mE, vE, beta_meanE, beta_varE = exact_solution(data, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "        mVB, vVB, beta_meanVB, beta_varVB = vb_solution(data, m0.value, v0.value, beta_mean0.value, beta_var0.value)\n",
    "        bE = beta_varE / beta_meanE\n",
    "        cE = (beta_meanE**2) / beta_varE\n",
    "        stdE = np.sqrt(vE / (bE * (cE - 1)))\n",
    "        stdVB = np.sqrt(vVB)\n",
    "        \n",
    "        # append to list\n",
    "        meanE.append(mE)\n",
    "        standard_devE.append(stdE)\n",
    "        meanVB.append(mVB)\n",
    "        standard_devVB.append(stdVB)\n",
    "        \n",
    "    # take mean across all runs\n",
    "    mE, stdE = [np.mean(s) for s in (meanE, standard_devE)]\n",
    "    mVB, stdVB = [np.mean(s) for s in (meanVB, standard_devVB)]\n",
    "    \n",
    "    # get range of points on x-axis\n",
    "    x = get_x_range((mE, stdE), (mVB, stdVB), (mu.value, std.value))\n",
    "    \n",
    "    # get contours\n",
    "    yE = get_contour(x, mE, stdE)\n",
    "    yVB = get_contour(x, mVB, stdVB)\n",
    "    ydata = get_contour(x, mu.value, std.value)\n",
    "    \n",
    "    # plot\n",
    "    plot_contours(ax5,\n",
    "                  (x, yE, colours[\"exact\"], \"Exact\", linestyles[\"exact\"]),\n",
    "                  (x, yVB, colours[\"approx\"], \"Approximation\", linestyles[\"approx\"]))\n",
    "\n",
    "def update_contour_plot(change):\n",
    "    ax5.clear()\n",
    "    plot_vb_and_exact_contours(ax5)\n",
    "\n",
    "fig5, ax5 = plt.subplots(num=5)\n",
    "plot_vb_and_exact_contours(ax5)\n",
    "\n",
    "display(grid);\n",
    "\n",
    "[w.children[n].observe(update_contour_plot, 'value') for n in range(len(w.children))];\n",
    "[prior_widget.children[n].observe(update_contour_plot, 'value') for n in range(len(prior_widget.children))];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
